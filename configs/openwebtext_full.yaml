# Full-scale training on OpenWebText
# Production configuration for serious training runs

dataset: openwebtext
batch_size: 8
gradient_accumulation_steps: 8  # Effective batch size = 64
num_epochs: 1  # OpenWebText is large, 1 epoch is sufficient
max_seq_length: 1024

# Sharpness penalty
penalty_type: non_max_sum
penalty_weight: 0.01

# Optimization
learning_rate: 0.0006
weight_decay: 0.1
warmup_steps: 2000
max_grad_norm: 1.0
betas: [0.9, 0.95]

# Logging and checkpointing
use_wandb: true
wandb_project: gpt2-entropy-regularization
wandb_entity: null  # Set to your W&B username
log_interval: 50
eval_interval: 1000
save_interval: 5000
generate_interval: 1000
num_generate_samples: 5

# System
device: cuda
seed: 42
num_workers: 4

# Output
output_dir: ./outputs/openwebtext
checkpoint_dir: ./checkpoints/openwebtext
