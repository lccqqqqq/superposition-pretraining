# Experiment: Negative entropy penalty
# Minimizes entropy to encourage concentrated probability distributions

dataset: tinyshakespeare
batch_size: 8
gradient_accumulation_steps: 4
num_epochs: 10

# Negative entropy penalty
penalty_type: neg_entropy
penalty_weight: 0.01

# W&B settings
use_wandb: true
wandb_project: gpt2-entropy-regularization
wandb_entity: null

# Standard training settings
learning_rate: 0.0006
max_seq_length: 1024
seed: 42
